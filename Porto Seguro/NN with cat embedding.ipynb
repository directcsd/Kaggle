{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# limit tensorflow GPU memory usage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "#random seeds for stochastic parts of neural network \n",
    "np.random.seed(43)\n",
    "tf.set_random_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Merge, Reshape, Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.gz')\n",
    "df_test = pd.read_csv('test.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.iloc[:,3:], df_train.target\n",
    "X_test = df_test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_use = [c for c in X_train.columns if (not c.startswith('ps_calc_'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[cols_use]\n",
    "X_test = X_test[cols_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_vals_dict = {c: list(X_train[c].unique()) for c in X_train.columns if c.endswith('_cat')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_02_cat: 5 values\n",
      "ps_ind_04_cat: 3 values\n",
      "ps_ind_05_cat: 8 values\n",
      "ps_car_01_cat: 13 values\n",
      "ps_car_02_cat: 3 values\n",
      "ps_car_03_cat: 3 values\n",
      "ps_car_04_cat: 10 values\n",
      "ps_car_05_cat: 3 values\n",
      "ps_car_06_cat: 18 values\n",
      "ps_car_07_cat: 3 values\n",
      "ps_car_09_cat: 6 values\n",
      "ps_car_10_cat: 3 values\n",
      "ps_car_11_cat: 104 values\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed_cols = []\n",
    "for c in col_vals_dict:\n",
    "    if len(col_vals_dict[c])>2:\n",
    "        embed_cols.append(c)\n",
    "        print(c + ': %d values' % len(col_vals_dict[c])) #look at value counts to know the embedding dimensions\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_network():\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    model_ps_ind_02_cat = Sequential()\n",
    "    model_ps_ind_02_cat.add(Embedding(5, 3, input_length=1))\n",
    "    model_ps_ind_02_cat.add(Reshape(target_shape=(3,)))\n",
    "    models.append(model_ps_ind_02_cat)\n",
    "    \n",
    "    model_ps_ind_04_cat = Sequential()\n",
    "    model_ps_ind_04_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_ind_04_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_ind_04_cat)\n",
    "    \n",
    "    model_ps_ind_05_cat = Sequential()\n",
    "    model_ps_ind_05_cat.add(Embedding(8, 5, input_length=1))\n",
    "    model_ps_ind_05_cat.add(Reshape(target_shape=(5,)))\n",
    "    models.append(model_ps_ind_05_cat)\n",
    "    \n",
    "    model_ps_car_01_cat = Sequential()\n",
    "    model_ps_car_01_cat.add(Embedding(13, 7, input_length=1))\n",
    "    model_ps_car_01_cat.add(Reshape(target_shape=(7,)))\n",
    "    models.append(model_ps_car_01_cat)\n",
    "    \n",
    "    model_ps_car_02_cat = Sequential()\n",
    "    model_ps_car_02_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_02_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_02_cat)\n",
    "    \n",
    "    model_ps_car_03_cat = Sequential()\n",
    "    model_ps_car_03_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_03_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_03_cat)\n",
    "    \n",
    "    model_ps_car_04_cat = Sequential()\n",
    "    model_ps_car_04_cat.add(Embedding(10, 5, input_length=1))\n",
    "    model_ps_car_04_cat.add(Reshape(target_shape=(5,)))\n",
    "    models.append(model_ps_car_04_cat)\n",
    "    \n",
    "    model_ps_car_05_cat = Sequential()\n",
    "    model_ps_car_05_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_05_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_05_cat)\n",
    "    \n",
    "    model_ps_car_06_cat = Sequential()\n",
    "    model_ps_car_06_cat.add(Embedding(18, 8, input_length=1))\n",
    "    model_ps_car_06_cat.add(Reshape(target_shape=(8,)))\n",
    "    models.append(model_ps_car_06_cat)\n",
    "    \n",
    "    model_ps_car_07_cat = Sequential()\n",
    "    model_ps_car_07_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_07_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_07_cat)\n",
    "    \n",
    "    model_ps_car_09_cat = Sequential()\n",
    "    model_ps_car_09_cat.add(Embedding(6, 3, input_length=1))\n",
    "    model_ps_car_09_cat.add(Reshape(target_shape=(3,)))\n",
    "    models.append(model_ps_car_09_cat)\n",
    "    \n",
    "    model_ps_car_10_cat = Sequential()\n",
    "    model_ps_car_10_cat.add(Embedding(3, 2, input_length=1))\n",
    "    model_ps_car_10_cat.add(Reshape(target_shape=(2,)))\n",
    "    models.append(model_ps_car_10_cat)\n",
    "    \n",
    "    model_ps_car_11_cat = Sequential()\n",
    "    model_ps_car_11_cat.add(Embedding(104, 10, input_length=1))\n",
    "    model_ps_car_11_cat.add(Reshape(target_shape=(10,)))\n",
    "    models.append(model_ps_car_11_cat)\n",
    "    \n",
    "    model_rest = Sequential()\n",
    "    model_rest.add(Dense(16, input_dim=24))\n",
    "    models.append(model_rest)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Merge(models, mode='concat'))\n",
    "    model.add(Dense(80))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting data to list format to match the network structure\n",
    "def preproc(X_train, X_val, X_test):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_val = []\n",
    "    input_list_test = []\n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(X_train[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(X_train[c].map(val_map).values)\n",
    "        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n",
    "     \n",
    "    #the rest of the columns\n",
    "    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n",
    "    input_list_train.append(X_train[other_cols].values)\n",
    "    input_list_val.append(X_val[other_cols].values)\n",
    "    input_list_test.append(X_test[other_cols].values)\n",
    "    \n",
    "    return input_list_train, input_list_val, input_list_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gini scoring function from kernel at: \n",
    "#https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "def ginic(actual, pred):\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n",
    "    return giniSum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini_normalizedc(a, p):\n",
    "    return ginic(a, p) / ginic(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#network training\n",
    "K = 8\n",
    "runs_per_fold = 3\n",
    "n_epochs = 15\n",
    "\n",
    "cv_ginis = []\n",
    "full_val_preds = np.zeros(np.shape(X_train)[0])\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K, \n",
    "                            random_state = 231, \n",
    "                            shuffle = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 prediction cv gini: 0.28914\n",
      "\n",
      "\n",
      "Fold 1 prediction cv gini: 0.25813\n",
      "\n",
      "\n",
      "Fold 2 prediction cv gini: 0.28737\n",
      "\n",
      "\n",
      "Fold 3 prediction cv gini: 0.27478\n",
      "\n",
      "\n",
      "Fold 4 prediction cv gini: 0.26562\n",
      "\n",
      "\n",
      "Fold 5 prediction cv gini: 0.28751\n",
      "\n",
      "\n",
      "Fold 6 prediction cv gini: 0.27874\n",
      "\n",
      "\n",
      "Fold 7 prediction cv gini: 0.28062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n",
    "    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n",
    "    \n",
    "    X_test_f = X_test.copy()\n",
    "    \n",
    "    #upsampling adapted from kernel: \n",
    "    #https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "    pos = (pd.Series(y_train_f == 1))\n",
    "    \n",
    "    # Add positive examples\n",
    "    X_train_f = pd.concat([X_train_f, X_train_f.loc[pos]], axis=0)\n",
    "    y_train_f = pd.concat([y_train_f, y_train_f.loc[pos]], axis=0)\n",
    "    \n",
    "    # Shuffle data\n",
    "    idx = np.arange(len(X_train_f))\n",
    "    np.random.shuffle(idx)\n",
    "    X_train_f = X_train_f.iloc[idx]\n",
    "    y_train_f = y_train_f.iloc[idx]\n",
    "    \n",
    "    #preprocessing\n",
    "    proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train_f, X_val_f, X_test_f)\n",
    "    \n",
    "    #track oof prediction for cv scores\n",
    "    val_preds = 0\n",
    "    \n",
    "    for j in range(runs_per_fold):\n",
    "    \n",
    "        NN = build_embedding_network()\n",
    "        NN.fit(proc_X_train_f, y_train_f.values, epochs=n_epochs, batch_size=4096)\n",
    "   \n",
    "        val_preds += NN.predict(proc_X_val_f)[:,0] / runs_per_fold\n",
    "        y_preds[:,i] += NN.predict(proc_X_test_f)[:,0] / runs_per_fold\n",
    "        \n",
    "    full_val_preds[outf_ind] += val_preds\n",
    "        \n",
    "    cv_gini = gini_normalizedc(y_val_f.values, val_preds)\n",
    "    cv_ginis.append(cv_gini)\n",
    "    print ('\\nFold %i prediction cv gini: %.5f\\n' %(i,cv_gini))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean out of fold gini: 0.27774\n",
      "Full validation gini: 0.27738\n"
     ]
    }
   ],
   "source": [
    "print('Mean out of fold gini: %.5f' % np.mean(cv_ginis))\n",
    "print('Full validation gini: %.5f' % gini_normalizedc(y_train.values, full_val_preds))\n",
    "\n",
    "y_pred_final = np.mean(y_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({'id' : df_test.id, \n",
    "                       'target' : y_pred_final},\n",
    "                       columns = ['id','target'])\n",
    "df_sub.to_csv('NN_EntityEmbed_10fold-sub.csv', index=False)\n",
    "\n",
    "pd.DataFrame(full_val_preds).to_csv('NN_EntityEmbed_10fold-val_preds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "class WeightedBinaryCrossEntropy(object):\n",
    "\n",
    "    def __init__(self, pos_ratio):\n",
    "        neg_ratio = 1. - pos_ratio\n",
    "        self.pos_ratio = tf.constant(pos_ratio, tf.float32)\n",
    "        self.weights = tf.constant(neg_ratio / pos_ratio, tf.float32)\n",
    "        self.__name__ = \"weighted_binary_crossentropy({0})\".format(pos_ratio)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.weighted_binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def weighted_binary_crossentropy(self, y_true, y_pred):\n",
    "            # Transform to logits\n",
    "            epsilon = tf.convert_to_tensor(K.common._EPSILON, y_pred.dtype.base_dtype)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "            y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "\n",
    "            cost = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.weights)\n",
    "            return K.mean(cost * self.pos_ratio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Autoencoder to train first 2 layers\n",
    "inputs=tf.keras.layers.Input(shape=(133,))\n",
    "#x=tf.keras.layers.Dense(37, kernel_initializer='he_normal', bias_initializer='he_normal')(inputs)\n",
    "#x=tf.keras.layers.ELU()(x)\n",
    "#x=tf.keras.layers.BatchNormalization()(x)\n",
    "#x=tf.keras.layers.Dropout(0.5)(x)\n",
    "#x=tf.keras.layers.add([inputs,x])\n",
    "encoded=tf.keras.layers.Dense(8000, kernel_initializer='he_normal', bias_initializer='he_normal')(inputs)\n",
    "encoded=tf.keras.layers.ELU()(encoded)\n",
    "encoded=tf.keras.layers.BatchNormalization()(encoded)\n",
    "encoded=tf.keras.layers.Dropout(0.5)(encoded)\n",
    "encoded=tf.keras.layers.Dense(1000, kernel_initializer='he_normal', bias_initializer='he_normal')(encoded)\n",
    "encoded=tf.keras.layers.ELU()(encoded)\n",
    "encoded=tf.keras.layers.BatchNormalization()(encoded)\n",
    "encoded=tf.keras.layers.Dropout(0.5)(encoded)\n",
    "\n",
    "decoded=tf.keras.layers.Dense(1000, kernel_initializer='he_normal', bias_initializer='he_normal')(encoded)\n",
    "decoded=tf.keras.layers.ELU()(decoded)\n",
    "decoded=tf.keras.layers.BatchNormalization()(decoded)\n",
    "decoded=tf.keras.layers.Dropout(0.5)(decoded)\n",
    "decoded=tf.keras.layers.Dense(8000, kernel_initializer='he_normal', bias_initializer='he_normal')(decoded)\n",
    "decoded=tf.keras.layers.ELU()(decoded)\n",
    "decoded=tf.keras.layers.BatchNormalization()(decoded)\n",
    "decoded=tf.keras.layers.Dropout(0.5)(decoded)\n",
    "decoded=tf.keras.layers.Dense(133, activation='sigmoid')(decoded)\n",
    "#loss=WeightedBinaryCrossEntropy(0.036)\n",
    "autoencoder = tf.keras.models.Model(inputs, decoded)\n",
    "autoencoder.compile(optimizer='nadam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    %time autoencoder.fit(combined_scaled, combined_scaled, epochs=10, batch_size=1000, shuffle=True, callbacks=[tensorboard], validation_data=(x2_scaled, x2_scaled))\n",
    "    #save model\n",
    "    autoencoder_json = autoencoder.to_json()\n",
    "    with open(\"nn_autoencoder.json\", \"w\") as json_file:\n",
    "        json_file.write(autoencoder_json)\n",
    "    # serialize weights to HDF5\n",
    "    autoencoder.save_weights(\"nn_autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze encoder layer and predict\n",
    "encoded.trainable=False\n",
    "x=tf.keras.layers.Dense(500, kernel_initializer='he_normal', bias_initializer='he_normal')(encoded)\n",
    "x=tf.keras.layers.ELU()(x)\n",
    "x=tf.keras.layers.BatchNormalization()(x)\n",
    "x=tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions=tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model=tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "loss=WeightedBinaryCrossEntropy(0.036)\n",
    "model.compile(optimizer=tf.keras.optimizers.Nadam(),  \n",
    "              loss=loss,\n",
    "              #loss=pair_loss,\n",
    "              #optimizer=tf.keras.optimizers.SGD(lr=0.03, momentum=0.9, nesterov=True),\n",
    "              #metrics=['binary_accuracy']\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    %time temp=model.fit (x1_scaled, y1, validation_data=(x2_scaled,y2), callbacks=[tensorboard], epochs=10, batch_size=100)\n",
    "    %time prediction=model.predict(test_scaled)\n",
    "    %time val_prediction=model.predict(x2_scaled)\n",
    "    %time train_prediction=model.predict(x1_scaled)\n",
    "    scores=model.evaluate(x2_scaled,y2,verbose=1)\n",
    "    \n",
    "    #save model\n",
    "    model_json = model.to_json()\n",
    "    with open(\"nn_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"nn_model.h5\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
